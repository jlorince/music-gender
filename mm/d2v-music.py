from gensim.models.doc2vec import Doc2Vec,TaggedLineDocument,TaggedDocument
from gensim import utils
import gzip,os,glob,time,datetime,sys,argparse,platform,math
import numpy as np
from tqdm import tqdm as tq
from math import floor
import multiprocess as mp

help_string="""
THIS CODE IS ONLY TESTED AGAINST PYTHON 3.6!!!

This script generates Doc2Vec models from the last.fm music listening data

All model results are saved in a folder with name generated by the following (see arguments below):

"{}-{}-{}-{}-{}".format(size,window,min_count,sample)

Doc2Vec documentation: https://radimrehurek.com/gensim/models/doc2vec.html
"""


class timed(object):
    def __init__(self,desc='command',pad='',**kwargs):
        self.desc = desc
        self.kwargs = kwargs
        self.pad = pad
    def __enter__(self):
        self.start = time.time()
        print('{} started...'.format(self.desc))
    def __exit__(self, type, value, traceback):
        if len(self.kwargs)==0:
            print('{}{} complete in {}{}'.format(self.pad,self.desc,str(datetime.timedelta(seconds=time.time()-self.start)),self.pad))
        else:
            print('{}{} complete in {} ({}){}'.format(self.pad,self.desc,str(datetime.timedelta(seconds=time.time()-self.start)),','.join(['{}={}'.format(*kw) for kw in self.kwargs.iteritems()]),self.pad))

def get_songs(fi):
    songs = [line.split('\t')[0] for line in open(fi)]
    doc = ' '.join(songs)
    userid = fi[fi.rfind('\\')+1:-4]
    return userid,doc


if __name__ == '__main__':

    parser = argparse.ArgumentParser(help_string)
    parser.add_argument("--size", help="Dimensionality of D2V vectors (see Doc2Vec documentation).",type=int,default=100)
    parser.add_argument("--window", help="Doc2Vec window size (see Doc2Vec documentation).",type=int,default=5)
    parser.add_argument("--min_count", help="Mininum number of times a word (song) must occur to be included in model (see Doc2Vec documentation).",type=int,default=5)
    parser.add_argument("--sample", help="Threshold for configuring which higher-frequency words (songs) are randomly downsampled (see Doc2Vec documentation).",type=int,default=0)
    parser.add_argument("--workers", help="Number of workers to use in parallel computations. Defaults to output of mp.cpu_count()",type=int,default=mp.cpu_count())
    parser.add_argument("--preprocess", action='store_true',help="Perform initial preprocessing of raw data files.")
    parser.add_argument("--scrobble_dir", help="Location of the raw scrobble files (one file per user).",type=str,default='P:/Projects/BigMusic/scrobbles-complete/')
    parser.add_argument("--d2v_dir", help="Output directory. A new subfolder in this directory will be generated for each new model run",type=str,default='P:/Projects/BigMusic/D2V/')
    parser.add_argument("--norm", action='store_true',help="If provided, also generate l2-normed word and document vector arrays.")


    args = parser.parse_args()


    if args.preprocess:

        pool = mp.Pool(args.workers)
        files = [args.scrobble_dir+f for f in os.listdir(args.scrobble_dir)]
        n = len(files)
        chunksize = int(math.ceil(n / float(procs)))
        with gzip.open(base_output_path+'docs_songs.txt.gz','w') as fout, gzip.open(base_output_path+'indices.txt.gz','w') as indices:
            for userid,doc in tq(pool.imap_unordered(get_songs,files,chunksize=1000),total=n):
                fout.write(doc+'\n')
                indices.write(userid+'\n')


    documents = TaggedLineDocument(args.d2v_dir+'docs_songs.txt.gz')

    pathname = "{}-{}-{}-{}-{}".format(args.size,args.window,args.min_count,args.sample)
    if os.path.exists(args.d2v_dir+pathname):
        raise Exception("It appears this model has already been run.")
    else:
        os.mkdir(args.d2v_dir+pathname)
    if args.year_sample:
        np.save('{}{}/doc_indices_sampled_{}.npy'.format(args.d2v_dir,pathname),indices_to_write)
           


with timed('Running Doc2Vec'):
    model = Doc2Vec(documents, dm=1, sample=args.sample, size=args.size, window=args.window, min_count=args.min_count,workers=args.workers)

if args.normed:
    with timed('Norming vectors'):
        from sklearn.preprocessing import Normalizer
        nrm = Normalizer('l2')
        normed = nrm.fit_transform(model.docvecs.doctag_syn0)
        words_normed = nrm.fit_transform(model.wv.syn0)


with timed('Saving data'):
    if args.normed:        
        np.save('{0}{1}/user_features_normed_{1}.npy'.format(args.d2v_dir,pathname),normed)
        np.save('{0}{1}/song_features_normed_{1}.npy'.format(args.d2v_dir,pathname),words_normed)
    model.save('{0}{1}/model_{1}'.format(args.d2v_dir,pathname))
    with open('{0}{1}/song_indices_{1}'.format(args.d2v_dir,pathname),'w') as out:
        for song in model.wv.index2word:
            out.write(str(song)+'\n')

